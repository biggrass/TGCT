\section{Introduction}
Over the past decade, we have seen a tremendous rise in mobile applications. Given the fact that these applications are used to record and assist end users daily life, it becomes critical for developers to test Android applications and further improve software quality. However, testing on Android apps is becoming challenging due to the fragmentation issues of the Android platform\cite{han2012understanding,park2013fragmentation}, the complexity of usage scenarios and the fast iteration of the Android platform.

Crowdsourcing was proposed to solve the dilemma of mobile application testing. Crowdsourcing, a company or organization that outsources tasks that were performed by employees before to non-specific (and usually large) public volunteers\cite{brabham2008crowdsourcing,estelles2012towards}. The crowdsourced testing is that the requestors outsource the test tasks to different crowd workers, who fulfill the tasks and submit reports to the crowdsourcing test platform. The platform integrates the reports and feeds them back to the requestors\cite{feng2015test}. However, there are three inherent shortcomings of this test pattern. 1) Crowd workers have lower professional literacy. 2) The test report is difficult to integrate. 3) The process of crowdsourced testing lacks management. 
Providing test guidance for crowdsourcing workers can optimize crowdsourcing testing mechanism and alleviate the above problems. According to the idea of collective intelligence, we developed a tool, TGCT, which takes automated test results as testing tasks, and guides crowd workers to reproduce and verify anomalies in different equipment, different environment, and different scenarios. At the same time, in order to avoid the limitations of the automated test, the tool analyzes the Android program source code to get the window jump that is not covered by the automated test, which helps to guide crowdsourcing workers to explore new exception and improve overall test coverage. To verify the effectiveness of TGCT, we selected and analyzed three Android applications to complete the comparison experiment. We analyze the effectiveness of the guide mechanism and the necessity of static source code analysis from the perspective of test efficiency and code coverage. Experiment shows that the tool-guided crowdsourcing testing mechanism for mobile applications can effectively improve the quality of crowdsourcing testing.

In this paper, we mainly make the following contributions:
1.We firstly propose a cooperation platform, TGCT, for Android application testing, and apply guide mechanism to crowdsourcing testing process. 2.Comparing the quality of the crowdsourcing test results under the guide and non-guide mechanisms. 
3.Evaluate the mechanism using real data from the perspective of test efficiency and code coverage.


